// This file was auto-generated by ML.NET Model Builder. 

using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using Microsoft.ML;
using Microsoft.ML.Data;
using Kinect_Science_FairML.Model;

namespace Kinect_Science_FairML.ConsoleApp
{
    public static class ModelBuilder
    {
        private static string TRAIN_DATA_FILEPATH = @"C:\Kinect Science Fair\Depth Data\KinectDataMinMax.csv";
        private static string MODEL_FILEPATH = @"C:\Users\Jack's PC\AppData\Local\Temp\MLVSTools\Kinect Science FairML\Kinect Science FairML.Model\MLModel.zip";
        // Create MLContext to be shared across the model creation workflow objects 
        // Set a random seed for repeatable/deterministic results across multiple trainings.
        private static MLContext mlContext = new MLContext(seed: 1);

        public static void CreateModel()
        {
            // Load Data
            IDataView trainingDataView = mlContext.Data.LoadFromTextFile<ModelInput>(
                                            path: TRAIN_DATA_FILEPATH,
                                            hasHeader: false,
                                            separatorChar: ',',
                                            allowQuoting: true,
                                            allowSparse: false);

            // Build training pipeline
            IEstimator<ITransformer> trainingPipeline = BuildTrainingPipeline(mlContext);

            // Train Model
            ITransformer mlModel = TrainModel(mlContext, trainingDataView, trainingPipeline);

            // Evaluate quality of Model
            Evaluate(mlContext, trainingDataView, trainingPipeline);

            // Save model
            SaveModel(mlContext, mlModel, MODEL_FILEPATH, trainingDataView.Schema);
        }

        public static IEstimator<ITransformer> BuildTrainingPipeline(MLContext mlContext)
        {
            // Data process configuration with pipeline data transformations 
            var dataProcessPipeline = mlContext.Transforms.Conversion.MapValueToKey("col0", "col0")
                                      .Append(mlContext.Transforms.Concatenate("Features", new[] { "col1", "col2", "col3", "col4", "col5", "col6", "col7", "col8", "col9", "col10", "col11", "col12", "col13", "col14", "col15", "col16", "col17", "col18", "col19", "col20", "col21", "col22", "col23", "col24", "col25", "col26", "col27", "col28", "col29", "col30", "col31", "col32", "col33", "col34", "col35", "col36", "col37", "col38", "col39", "col40", "col41", "col42", "col43", "col44", "col45", "col46", "col47", "col48", "col49", "col50", "col51", "col52", "col53", "col54", "col55", "col56", "col57", "col58", "col59", "col60", "col61", "col62", "col63", "col64", "col65", "col66", "col67", "col68", "col69", "col70", "col71", "col72", "col73", "col74", "col75", "col76", "col77", "col78", "col79", "col80", "col81", "col82", "col83", "col84", "col85", "col86", "col87", "col88", "col89", "col90", "col91", "col92", "col93", "col94", "col95", "col96", "col97", "col98", "col99", "col100", "col101", "col102", "col103", "col104", "col105", "col106", "col107", "col108", "col109", "col110", "col111", "col112", "col113", "col114", "col115", "col116", "col117", "col118", "col119", "col120", "col121", "col122", "col123", "col124", "col125", "col126", "col127", "col128", "col129", "col130", "col131", "col132", "col133", "col134", "col135", "col136", "col137", "col138", "col139", "col140", "col141", "col142", "col143", "col144", "col145", "col146", "col147", "col148", "col149", "col150", "col151", "col152", "col153", "col154", "col155", "col156", "col157", "col158", "col159", "col160", "col161", "col162", "col163", "col164", "col165", "col166", "col167", "col168", "col169", "col170", "col171", "col172", "col173", "col174", "col175", "col176", "col177", "col178", "col179", "col180", "col181", "col182", "col183", "col184", "col185", "col186", "col187", "col188", "col189", "col190", "col191", "col192", "col193", "col194", "col195", "col196", "col197", "col198", "col199", "col200", "col201", "col202", "col203", "col204", "col205", "col206", "col207", "col208", "col209", "col210", "col211", "col212", "col213", "col214", "col215", "col216", "col217", "col218", "col219", "col220", "col221", "col222", "col223", "col224", "col225", "col226", "col227", "col228", "col229", "col230", "col231", "col232", "col233", "col234", "col235", "col236", "col237", "col238", "col239", "col240", "col241", "col242", "col243", "col244", "col245", "col246", "col247", "col248", "col249", "col250", "col251", "col252", "col253", "col254", "col255", "col256", "col257", "col258", "col259", "col260", "col261", "col262", "col263", "col264", "col265", "col266", "col267", "col268", "col269", "col270", "col271", "col272", "col273", "col274", "col275", "col276", "col277", "col278", "col279", "col280", "col281", "col282", "col283", "col284", "col285", "col286", "col287", "col288", "col289", "col290", "col291", "col292", "col293", "col294", "col295", "col296", "col297", "col298", "col299", "col300", "col301", "col302", "col303", "col304", "col305", "col306", "col307", "col308", "col309", "col310", "col311", "col312", "col313", "col314", "col315", "col316", "col317", "col318", "col319", "col320", "col321", "col322", "col323", "col324", "col325", "col326", "col327", "col328", "col329", "col330", "col331", "col332", "col333", "col334", "col335", "col336", "col337", "col338", "col339", "col340", "col341", "col342", "col343", "col344", "col345", "col346", "col347", "col348", "col349", "col350", "col351", "col352", "col353", "col354", "col355", "col356", "col357", "col358", "col359", "col360", "col361", "col362", "col363", "col364", "col365", "col366", "col367", "col368", "col369", "col370", "col371", "col372", "col373", "col374", "col375", "col376", "col377", "col378", "col379", "col380", "col381", "col382", "col383", "col384", "col385", "col386", "col387", "col388", "col389", "col390", "col391", "col392", "col393", "col394", "col395", "col396", "col397", "col398", "col399", "col400", "col401", "col402", "col403", "col404", "col405", "col406", "col407", "col408", "col409", "col410", "col411", "col412", "col413", "col414", "col415", "col416", "col417", "col418", "col419", "col420", "col421", "col422", "col423", "col424", "col425", "col426", "col427", "col428", "col429", "col430", "col431", "col432", "col433", "col434", "col435", "col436", "col437", "col438", "col439", "col440", "col441", "col442", "col443", "col444", "col445", "col446", "col447", "col448", "col449", "col450", "col451", "col452", "col453", "col454", "col455", "col456", "col457", "col458", "col459", "col460", "col461", "col462", "col463", "col464", "col465", "col466", "col467", "col468", "col469", "col470", "col471", "col472", "col473", "col474", "col475", "col476", "col477", "col478", "col479", "col480" }))
                                      .Append(mlContext.Transforms.NormalizeMinMax("Features", "Features"))
                                      .AppendCacheCheckpoint(mlContext);
            // Set the training algorithm 
            var trainer = mlContext.MulticlassClassification.Trainers.OneVersusAll(mlContext.BinaryClassification.Trainers.AveragedPerceptron(labelColumnName: "col0", numberOfIterations: 10, featureColumnName: "Features"), labelColumnName: "col0")
                                      .Append(mlContext.Transforms.Conversion.MapKeyToValue("PredictedLabel", "PredictedLabel"));

            var trainingPipeline = dataProcessPipeline.Append(trainer);

            return trainingPipeline;
        }

        public static ITransformer TrainModel(MLContext mlContext, IDataView trainingDataView, IEstimator<ITransformer> trainingPipeline)
        {
            Console.WriteLine("=============== Training  model ===============");

            ITransformer model = trainingPipeline.Fit(trainingDataView);

            Console.WriteLine("=============== End of training process ===============");
            return model;
        }

        private static void Evaluate(MLContext mlContext, IDataView trainingDataView, IEstimator<ITransformer> trainingPipeline)
        {
            // Cross-Validate with single dataset (since we don't have two datasets, one for training and for evaluate)
            // in order to evaluate and get the model's accuracy metrics
            Console.WriteLine("=============== Cross-validating to get model's accuracy metrics ===============");
            var crossValidationResults = mlContext.MulticlassClassification.CrossValidate(trainingDataView, trainingPipeline, numberOfFolds: 5, labelColumnName: "col0");
            PrintMulticlassClassificationFoldsAverageMetrics(crossValidationResults);
        }

        private static void SaveModel(MLContext mlContext, ITransformer mlModel, string modelRelativePath, DataViewSchema modelInputSchema)
        {
            // Save/persist the trained model to a .ZIP file
            Console.WriteLine($"=============== Saving the model  ===============");
            mlContext.Model.Save(mlModel, modelInputSchema, GetAbsolutePath(modelRelativePath));
            Console.WriteLine("The model is saved to {0}", GetAbsolutePath(modelRelativePath));
        }

        public static string GetAbsolutePath(string relativePath)
        {
            FileInfo _dataRoot = new FileInfo(typeof(Program).Assembly.Location);
            string assemblyFolderPath = _dataRoot.Directory.FullName;

            string fullPath = Path.Combine(assemblyFolderPath, relativePath);

            return fullPath;
        }

        public static void PrintMulticlassClassificationMetrics(MulticlassClassificationMetrics metrics)
        {
            Console.WriteLine($"************************************************************");
            Console.WriteLine($"*    Metrics for multi-class classification model   ");
            Console.WriteLine($"*-----------------------------------------------------------");
            Console.WriteLine($"    MacroAccuracy = {metrics.MacroAccuracy:0.####}, a value between 0 and 1, the closer to 1, the better");
            Console.WriteLine($"    MicroAccuracy = {metrics.MicroAccuracy:0.####}, a value between 0 and 1, the closer to 1, the better");
            Console.WriteLine($"    LogLoss = {metrics.LogLoss:0.####}, the closer to 0, the better");
            for (int i = 0; i < metrics.PerClassLogLoss.Count; i++)
            {
                Console.WriteLine($"    LogLoss for class {i + 1} = {metrics.PerClassLogLoss[i]:0.####}, the closer to 0, the better");
            }
            Console.WriteLine($"************************************************************");
        }

        public static void PrintMulticlassClassificationFoldsAverageMetrics(IEnumerable<TrainCatalogBase.CrossValidationResult<MulticlassClassificationMetrics>> crossValResults)
        {
            var metricsInMultipleFolds = crossValResults.Select(r => r.Metrics);

            var microAccuracyValues = metricsInMultipleFolds.Select(m => m.MicroAccuracy);
            var microAccuracyAverage = microAccuracyValues.Average();
            var microAccuraciesStdDeviation = CalculateStandardDeviation(microAccuracyValues);
            var microAccuraciesConfidenceInterval95 = CalculateConfidenceInterval95(microAccuracyValues);

            var macroAccuracyValues = metricsInMultipleFolds.Select(m => m.MacroAccuracy);
            var macroAccuracyAverage = macroAccuracyValues.Average();
            var macroAccuraciesStdDeviation = CalculateStandardDeviation(macroAccuracyValues);
            var macroAccuraciesConfidenceInterval95 = CalculateConfidenceInterval95(macroAccuracyValues);

            var logLossValues = metricsInMultipleFolds.Select(m => m.LogLoss);
            var logLossAverage = logLossValues.Average();
            var logLossStdDeviation = CalculateStandardDeviation(logLossValues);
            var logLossConfidenceInterval95 = CalculateConfidenceInterval95(logLossValues);

            var logLossReductionValues = metricsInMultipleFolds.Select(m => m.LogLossReduction);
            var logLossReductionAverage = logLossReductionValues.Average();
            var logLossReductionStdDeviation = CalculateStandardDeviation(logLossReductionValues);
            var logLossReductionConfidenceInterval95 = CalculateConfidenceInterval95(logLossReductionValues);

            Console.WriteLine($"*************************************************************************************************************");
            Console.WriteLine($"*       Metrics for Multi-class Classification model      ");
            Console.WriteLine($"*------------------------------------------------------------------------------------------------------------");
            Console.WriteLine($"*       Average MicroAccuracy:    {microAccuracyAverage:0.###}  - Standard deviation: ({microAccuraciesStdDeviation:#.###})  - Confidence Interval 95%: ({microAccuraciesConfidenceInterval95:#.###})");
            Console.WriteLine($"*       Average MacroAccuracy:    {macroAccuracyAverage:0.###}  - Standard deviation: ({macroAccuraciesStdDeviation:#.###})  - Confidence Interval 95%: ({macroAccuraciesConfidenceInterval95:#.###})");
            Console.WriteLine($"*       Average LogLoss:          {logLossAverage:#.###}  - Standard deviation: ({logLossStdDeviation:#.###})  - Confidence Interval 95%: ({logLossConfidenceInterval95:#.###})");
            Console.WriteLine($"*       Average LogLossReduction: {logLossReductionAverage:#.###}  - Standard deviation: ({logLossReductionStdDeviation:#.###})  - Confidence Interval 95%: ({logLossReductionConfidenceInterval95:#.###})");
            Console.WriteLine($"*************************************************************************************************************");

        }

        public static double CalculateStandardDeviation(IEnumerable<double> values)
        {
            double average = values.Average();
            double sumOfSquaresOfDifferences = values.Select(val => (val - average) * (val - average)).Sum();
            double standardDeviation = Math.Sqrt(sumOfSquaresOfDifferences / (values.Count() - 1));
            return standardDeviation;
        }

        public static double CalculateConfidenceInterval95(IEnumerable<double> values)
        {
            double confidenceInterval95 = 1.96 * CalculateStandardDeviation(values) / Math.Sqrt((values.Count() - 1));
            return confidenceInterval95;
        }
    }
}
